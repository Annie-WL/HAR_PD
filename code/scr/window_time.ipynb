{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20380,
     "status": "ok",
     "timestamp": 1758464087435,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "72uzN6L7TvB8",
    "outputId": "75ce3ce7-2dc2-43a4-90f5-f68f54865349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STvZh7oaT272"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2bgK2PwT4CG"
   },
   "source": [
    "#### tsfel + 4 \n",
    "change window sliding seconds (3, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10429,
     "status": "ok",
     "timestamp": 1758464105489,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "vVVFYFEQqkmT",
    "outputId": "90e5b57c-f6f6-49e1-c226-c5fd055f2538"
   },
   "outputs": [],
   "source": [
    "pip install tsfel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 40039,
     "status": "ok",
     "timestamp": 1758464152571,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "BUq8mXteilf-"
   },
   "outputs": [],
   "source": [
    "# TSFEL + 4 \n",
    "import os, re, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, f1_score, balanced_accuracy_score,\n",
    "                             precision_score, recall_score, classification_report,\n",
    "                             confusion_matrix, roc_auc_score)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tsfel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# settings\n",
    "FS = 40\n",
    "WINDOW_SEC = 10\n",
    "OVERLAP = 0.5\n",
    "BASE_IN = \"/content/drive/My Drive/final_project/smooth_data4\"\n",
    "BASE_OUT = f\"/content/drive/My Drive/final_project/segment_{WINDOW_SEC}s_tsfel4\"\n",
    "os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "def parse_fileinfo(path):\n",
    "    name = os.path.basename(path).replace(\".csv\",\"\")\n",
    "    sensor = \"wrist\" if re.search(r\"wrist\", name, re.I) else (\"ankle\" if re.search(r\"ankle\", name, re.I) else None)\n",
    "    m = re.search(r\"T(\\d+)_((M10[_-]?WT\\d+)|M\\d+)\", name, re.I)\n",
    "    if not m or sensor is None:\n",
    "        return None\n",
    "    subject = int(m.group(1))\n",
    "    activity = m.group(2).replace(\"_\",\"-\").upper().replace(\"M10-WT\",\"M10_WT\")\n",
    "    return subject, activity, sensor\n",
    "\n",
    "def spectral_centroid(f, pxx):\n",
    "    pxx = np.clip(pxx, 1e-12, None)\n",
    "    return float(np.sum(f * pxx) / np.sum(pxx))\n",
    "\n",
    "def spectral_edge_freq(f, pxx, edge=0.9):\n",
    "    p = np.cumsum(np.clip(pxx, 1e-12, None))\n",
    "    p = p / p[-1]\n",
    "    idx = np.searchsorted(p, edge)\n",
    "    return float(f[min(idx, len(f)-1)])\n",
    "\n",
    "def tsfel_feats_for_series(series, cfg):\n",
    "    df_tmp = tsfel.time_series_features_extractor(cfg, series.reset_index(drop=True), fs=FS, verbose=0)\n",
    "    return df_tmp.iloc[0].to_dict()\n",
    "\n",
    "def jerk_features_from_acc_mag(win_df, prefix=\"\"):\n",
    "    need = [\"Acc_X\",\"Acc_Y\",\"Acc_Z\"]\n",
    "    if not all([c in win_df.columns for c in need]):\n",
    "        return {}\n",
    "    ax, ay, az = [win_df[c].values.astype(float) for c in need]\n",
    "    amag = np.sqrt(ax*ax + ay*ay + az*az)\n",
    "    jerk = np.diff(amag) * FS\n",
    "    if len(jerk) == 0:\n",
    "        return {}\n",
    "    # jerk \n",
    "    j_mean = float(np.mean(jerk))\n",
    "    j_std  = float(np.std(jerk, ddof=1)) if len(jerk)>1 else 0.0\n",
    "    # 频域特征\n",
    "    f, pxx = welch(amag, fs=FS, nperseg=min(len(amag), 256))\n",
    "    sc = spectral_centroid(f, pxx)\n",
    "    sef90 = spectral_edge_freq(f, pxx, edge=0.9)\n",
    "    return {\n",
    "        f\"{prefix}jerk_mean_mag\": j_mean,\n",
    "        f\"{prefix}jerk_std_mag\": j_std,\n",
    "        f\"{prefix}spectral_centroid_acc_mag\": sc,\n",
    "        f\"{prefix}spectral_edge_freq90_acc_mag\": sef90\n",
    "    }\n",
    "\n",
    "def segment_and_extract_tsfel(grouped, window_size, step):\n",
    "    cfg = tsfel.get_features_by_domain() \n",
    "    rows = []\n",
    "    one_d_cols = [\n",
    "        \"Acc_X\",\"Acc_Y\",\"Acc_Z\",\n",
    "        \"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\",\n",
    "        \"FreeAcc_E\",\"FreeAcc_N\",\"FreeAcc_U\",\n",
    "        \"Roll\",\"Pitch\",\"Yaw\"\n",
    "    ]\n",
    "    for (subj, act), sensors in grouped.items():\n",
    "        if \"wrist\" not in sensors or \"ankle\" not in sensors:\n",
    "            continue\n",
    "        df_w = sensors[\"wrist\"]; df_a = sensors[\"ankle\"]\n",
    "        L = min(len(df_w), len(df_a))\n",
    "        if L < window_size:\n",
    "            continue\n",
    "        for widx, start in enumerate(range(0, L - window_size + 1, step), 1):\n",
    "            win_w = df_w.iloc[start:start+window_size]\n",
    "            win_a = df_a.iloc[start:start+window_size]\n",
    "            feats = {}\n",
    "            # Ankle TSFEL\n",
    "            for col in one_d_cols:\n",
    "                if col in win_a.columns:\n",
    "                    d = tsfel_feats_for_series(win_a[col], cfg)\n",
    "                    feats.update({f\"ankle_{col}__{k}\": float(v) for k,v in d.items()})\n",
    "            feats.update(jerk_features_from_acc_mag(win_a, prefix=\"ankle_\"))\n",
    "            # Wrist TSFEL\n",
    "            for col in one_d_cols:\n",
    "                if col in win_w.columns:\n",
    "                    d = tsfel_feats_for_series(win_w[col], cfg)\n",
    "                    feats.update({f\"wrist_{col}__{k}\": float(v) for k,v in d.items()})\n",
    "            feats.update(jerk_features_from_acc_mag(win_w, prefix=\"wrist_\"))\n",
    "\n",
    "            feats[\"window\"] = widx\n",
    "            feats[\"Subject\"] = subj\n",
    "            feats[\"Activity\"] = act\n",
    "            rows.append(feats)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def ensure_dirs():\n",
    "    os.makedirs(f\"{BASE_OUT}/features\", exist_ok=True)\n",
    "    os.makedirs(f\"{BASE_OUT}/gender\", exist_ok=True)\n",
    "    os.makedirs(f\"{BASE_OUT}/gender_merged\", exist_ok=True)\n",
    "    os.makedirs(f\"{BASE_OUT}/figures\", exist_ok=True)\n",
    "\n",
    "# read file\n",
    "ensure_dirs()\n",
    "files = sorted(glob.glob(os.path.join(BASE_IN, \"*.csv\")))\n",
    "grouped = defaultdict(dict)\n",
    "for fp in files:\n",
    "    info = parse_fileinfo(fp)\n",
    "    if info is None:\n",
    "        continue\n",
    "    s, a, sensor = info\n",
    "    df = pd.read_csv(fp)\n",
    "    grouped[(s,a)][sensor] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2741307,
     "status": "ok",
     "timestamp": 1758466893858,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "SopBZGntmZfG",
    "outputId": "4fdf214f-8960-4b32-b1f9-c17cf26ad1e6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# window sliding & TSFEL+4 features extraction\n",
    "window_size = int(WINDOW_SEC * FS)\n",
    "step = max(1, int(window_size * (1-OVERLAP)))  # 50% overlap\n",
    "features_df = segment_and_extract_tsfel(grouped, window_size, step)\n",
    "feat_path = f\"{BASE_OUT}/features/all_features_merged.csv\"\n",
    "features_df.to_csv(feat_path, index=False)\n",
    "print(\"Saved:\", feat_path, \"shape:\", features_df.shape)\n",
    "\n",
    "# merge demographicss \n",
    "demo_xlsx = \"/content/drive/My Drive/final_project/participant_demographics_table.xlsx\"\n",
    "demo = pd.read_excel(demo_xlsx)\n",
    "demo.columns = demo.columns.str.strip().str.replace(\" \", \"_\").str.lower()\n",
    "if \"trial_id\" in demo.columns:\n",
    "    demo[\"subject\"] = demo[\"trial_id\"].str.extract(r\"T(\\d+)\", expand=False).astype(int)\n",
    "elif \"subject\" in demo.columns and demo[\"subject\"].dtype == object:\n",
    "    demo[\"subject\"] = demo[\"subject\"].str.extract(r\"T?(\\d+)\", expand=False).astype(int)\n",
    "\n",
    "merged = features_df.merge(demo, left_on=\"Subject\", right_on=\"subject\", how=\"left\")\n",
    "merged_path = f\"{BASE_OUT}/features/features_with_demo.csv\"\n",
    "merged.to_csv(merged_path, index=False)\n",
    "\n",
    "act_counts = merged.groupby(\"Subject\")[\"Activity\"].nunique().reset_index()\n",
    "full_subjects = act_counts[act_counts[\"Activity\"] >= 12][\"Subject\"].tolist()\n",
    "\n",
    "subj_df = merged.groupby(\"Subject\").agg(\n",
    "    sex=(\"sex\",\"first\"),\n",
    "    stage=(\"hoehn_and_yahr_(stage)\",\"first\")\n",
    ").reset_index()\n",
    "subj_df[\"sex\"] = subj_df[\"sex\"].fillna(\"Unknown\").astype(str)\n",
    "subj_df[\"stage\"] = subj_df[\"stage\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "full_df = subj_df[subj_df[\"Subject\"].isin(full_subjects)]\n",
    "n_total = subj_df[\"Subject\"].nunique()\n",
    "n_test  = max(1, round(n_total * 0.2))\n",
    "if len(full_df) >= n_test:\n",
    "    test_subj_df, _ = train_test_split(full_df, test_size=len(full_df)-n_test,\n",
    "                                       stratify=full_df[\"sex\"], random_state=42)\n",
    "else:\n",
    "    test_subj_df, _ = train_test_split(subj_df, test_size=n_total-n_test,\n",
    "                                       stratify=subj_df[\"sex\"], random_state=42)\n",
    "\n",
    "remain = subj_df[~subj_df[\"Subject\"].isin(test_subj_df[\"Subject\"])]\n",
    "train_subj_df, val_subj_df = train_test_split(\n",
    "    remain, test_size=0.25, stratify=remain[\"sex\"], random_state=42\n",
    ")\n",
    "\n",
    "df = merged.copy()\n",
    "df[\"set\"] = \"train\"\n",
    "df.loc[df[\"Subject\"].isin(val_subj_df[\"Subject\"]), \"set\"] = \"val\"\n",
    "df.loc[df[\"Subject\"].isin(test_subj_df[\"Subject\"]), \"set\"] = \"test\"\n",
    "df.to_csv(f\"{BASE_OUT}/gender/features_with_demo_split.csv\", index=False)\n",
    "\n",
    "m10_map = {f\"M10_WT{i}\":\"M10\" for i in range(6)}\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    part = df[df[\"set\"]==split].copy()\n",
    "    part[\"Activity\"] = part[\"Activity\"].replace(m10_map)\n",
    "    part.to_csv(f\"{BASE_OUT}/gender_merged/{split}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2663501,
     "status": "ok",
     "timestamp": 1758483363499,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "9RP95Y9KmoAt",
    "outputId": "d1e36ecd-4834-4b8c-b043-7c141665b1f4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train/ val & LOSO\n",
    "train_df = pd.read_csv(f\"{BASE_OUT}/gender_merged/train.csv\")\n",
    "val_df   = pd.read_csv(f\"{BASE_OUT}/gender_merged/val.csv\")\n",
    "\n",
    "exclude_cols = [\n",
    "    \"Activity\",\"Subject\",\"window\",\"set\",\"trial_id\",\"subject\",\n",
    "    \"sex\",\"most_affected_wrist\",\"most_affected_ankle\",\"dominant_side\",\n",
    "    \"age\",\"height_(cm)\",\"weight_(kg)\",\"most_affected_side\",\n",
    "    \"hoehn_and_yahr_(stage)\",\"years_since_diagnosis\",\"cit\",\"updrs\"\n",
    "]\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[\"Activity\"]\n",
    "X_val,   y_val   = val_df[feature_cols],   val_df[\"Activity\"]\n",
    "\n",
    "d = X_train.shape[1]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"kbest\", SelectKBest(score_func=f_classif)),\n",
    "    # (\"clf\", RandomForestClassifier(class_weight=\"balanced\", random_state=42))\n",
    "    (\"clf\", HistGradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        early_stopping=True,  \n",
    "        validation_fraction=0.1,  \n",
    "        n_iter_no_change=10\n",
    "    ))\n",
    "\n",
    "])\n",
    "\n",
    "# RF\n",
    "# param_grid = {\n",
    "#     \"kbest__k\": [100, 150, 200],    \n",
    "#     \"clf__n_estimators\": [200, 250],\n",
    "#     \"clf__max_depth\": [2, 3, 5, 10],\n",
    "#     \"clf__min_samples_leaf\": [2, 4, 8],\n",
    "#     \"clf__min_samples_split\": [24, 8],\n",
    "#     \"clf__max_features\": [\"sqrt\",\"log2\"],     \n",
    "#     \"clf__bootstrap\": [True], \n",
    "#     \"clf__ccp_alpha\": [0.1] \n",
    "# }\n",
    "\n",
    "# HGB\n",
    "param_grid = {\n",
    "    \"kbest__k\": [70, 100, 150, 200],\n",
    "    \"clf__learning_rate\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.2],   \n",
    "    \"clf__max_iter\": [100, 200, 300],   \n",
    "    \"clf__max_depth\": [2, 3, 5, 7, 10],  \n",
    "    \"clf__min_samples_leaf\": [2, 3, 10, 30], \n",
    "    \"clf__l2_regularization\": [0.1, 0.5, 10.0, 20.0, 60.0], \n",
    "    \"clf__max_bins\": [128, 255],\n",
    "}\n",
    "\n",
    "\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "gs = GridSearchCV(pipe, param_grid, cv=gkf, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "gs.fit(X_train, y_train, groups=train_df[\"Subject\"])\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "best = gs.best_estimator_\n",
    "\n",
    "def all_metrics(model, X_tr, y_tr, X_va, y_va, name_prefix=\"\"):\n",
    "    ytr_pred = model.predict(X_tr)\n",
    "    yva_pred = model.predict(X_va)\n",
    "\n",
    "    yva_proba = model.predict_proba(X_va) if hasattr(model,\"predict_proba\") else None\n",
    "\n",
    "    metrics = {}\n",
    "    metrics[f\"{name_prefix}Train Accuracy\"] = accuracy_score(y_tr, ytr_pred)\n",
    "    metrics[f\"{name_prefix}Train Macro F1\"] = f1_score(y_tr, ytr_pred, average=\"macro\")\n",
    "    metrics[f\"{name_prefix}Train Balanced Accuracy\"] = balanced_accuracy_score(y_tr, ytr_pred)\n",
    "    metrics[f\"{name_prefix}Train Macro Precision\"] = precision_score(y_tr, ytr_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    metrics[f\"{name_prefix}Val Accuracy\"] = accuracy_score(y_va, yva_pred)\n",
    "    metrics[f\"{name_prefix}Val Macro F1\"] = f1_score(y_va, yva_pred, average=\"macro\")\n",
    "    metrics[f\"{name_prefix}Val Balanced Accuracy\"] = balanced_accuracy_score(y_va, yva_pred)\n",
    "    metrics[f\"{name_prefix}Val Macro Precision\"] = precision_score(y_va, yva_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    if yva_proba is not None:\n",
    "        lb = LabelBinarizer().fit(y_va)\n",
    "        y_true_bin = lb.transform(y_va)\n",
    "        try:\n",
    "            metrics[f\"{name_prefix}Val Macro ROC-AUC\"] = roc_auc_score(y_true_bin, yva_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "        except Exception:\n",
    "            metrics[f\"{name_prefix}Val Macro ROC-AUC\"] = np.nan\n",
    "\n",
    "    print(pd.Series(metrics).to_string())\n",
    "    print(\"\\nValidation Classification Report:\")\n",
    "    print(classification_report(y_va, yva_pred))\n",
    "\n",
    "    labels = sorted(y_va.unique())\n",
    "    cm = confusion_matrix(y_va, yva_pred, labels=labels)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Confusion Matrix (Validation)\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.xticks(rotation=45, ha=\"right\"); plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    fig_path = f\"{BASE_OUT}/figures/cm_val_{WINDOW_SEC}s.png\"\n",
    "    os.makedirs(os.path.dirname(fig_path), exist_ok=True)\n",
    "    plt.savefig(fig_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", fig_path)\n",
    "    return metrics\n",
    "\n",
    "best.fit(X_train, y_train)\n",
    "_ = all_metrics(best, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# LOSO\n",
    "comb = pd.concat([train_df, val_df], ignore_index=True)\n",
    "X = comb[feature_cols]; y = comb[\"Activity\"]; groups = comb[\"Subject\"]\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "fold_acc, fold_f1, fold_balacc = [], [], []\n",
    "y_true_all, y_pred_all = [], []\n",
    "\n",
    "for tr_idx, va_idx in logo.split(X, y, groups):\n",
    "    Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "    loso_model = Pipeline([\n",
    "        (\"scaler\", best.named_steps[\"scaler\"]),\n",
    "        (\"kbest\", SelectKBest(f_classif, k=best.named_steps[\"kbest\"].k)),\n",
    "        # (\"clf\", RandomForestClassifier(\n",
    "        #     n_estimators=best.named_steps[\"clf\"].n_estimators,\n",
    "        #     max_depth=best.named_steps[\"clf\"].max_depth,\n",
    "        #     min_samples_leaf=best.named_steps[\"clf\"].min_samples_leaf,\n",
    "        #     max_features=best.named_steps[\"clf\"].max_features,\n",
    "        #     class_weight=\"balanced\",\n",
    "        #     random_state=42\n",
    "        # ))\n",
    "        (\"clf\", HistGradientBoostingClassifier(\n",
    "        learning_rate=best.named_steps[\"clf\"].learning_rate,\n",
    "        max_iter=best.named_steps[\"clf\"].max_iter,\n",
    "        max_depth=best.named_steps[\"clf\"].max_depth,\n",
    "        min_samples_leaf=best.named_steps[\"clf\"].min_samples_leaf,\n",
    "        l2_regularization=best.named_steps[\"clf\"].l2_regularization,\n",
    "        max_bins=best.named_steps[\"clf\"].max_bins,\n",
    "        early_stopping=True, \n",
    "        validation_fraction=0.1,  \n",
    "        n_iter_no_change=10,\n",
    "        random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    loso_model.fit(Xtr, ytr)\n",
    "    yp = loso_model.predict(Xva)\n",
    "\n",
    "    fold_acc.append(accuracy_score(yva, yp))\n",
    "    fold_f1.append(f1_score(yva, yp, average=\"macro\"))\n",
    "    fold_balacc.append(balanced_accuracy_score(yva, yp))\n",
    "    y_true_all.extend(yva); y_pred_all.extend(yp)\n",
    "\n",
    "print(f\"LOSO Accuracy: {np.mean(fold_acc):.4f} ± {np.std(fold_acc):.4f}\")\n",
    "print(f\"LOSO Macro F1: {np.mean(fold_f1):.4f} ± {np.std(fold_f1):.4f}\")\n",
    "print(f\"LOSO Balanced Acc: {np.mean(fold_balacc):.4f} ± {np.std(fold_balacc):.4f}\")\n",
    "\n",
    "labels = sorted(comb[\"Activity\"].unique())\n",
    "cm = confusion_matrix(y_true_all, y_pred_all, labels=labels)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f\"LOSO Confusion Matrix ({WINDOW_SEC}s windows)\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "loso_fig = f\"{BASE_OUT}/figures/cm_loso_{WINDOW_SEC}s.png\"\n",
    "plt.savefig(loso_fig, dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved:\", loso_fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOydz4L5kno/WqtjxA9dpTc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
