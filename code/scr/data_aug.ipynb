{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56868,
     "status": "ok",
     "timestamp": 1757274513309,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "uRAo-2RMrCZ7",
    "outputId": "3f839a03-7810-4b57-bcb9-6e00a1c2ec2c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # 挂载 Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11599,
     "status": "ok",
     "timestamp": 1757274524902,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "TW-WeXqnq8PB",
    "outputId": "1fba31a6-38d8-4568-f786-4d2e075d0eef"
   },
   "outputs": [],
   "source": [
    "pip install tsfel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "PRIMARY_SIGNALS = [\n",
    "    \"Acc_X\", \"Acc_Y\", \"Acc_Z\",\n",
    "    \"Gyr_X\", \"Gyr_Y\", \"Gyr_Z\",\n",
    "    \"FreeAcc_E\", \"FreeAcc_N\", \"FreeAcc_U\",]\n",
    "\n",
    "def median_filter(\n",
    "    cleaned_folder=\"/content/drive/My Drive/final_project/augmentation/cleaned\",\n",
    "    output_folder=\"/content/drive/My Drive/final_project/augmentation/medianfilter\",\n",
    "    kernel_size=3,\n",
    "    return_dict=True\n",
    "):\n",
    "\n",
    "    assert kernel_size % 2 == 1, \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    filtered_data = {} if return_dict else None\n",
    "    files = sorted(glob.glob(os.path.join(cleaned_folder, \"*.csv\")))\n",
    "\n",
    "    for fp in files:\n",
    "        df = pd.read_csv(fp)\n",
    "\n",
    "        cols_to_filter = [c for c in PRIMARY_SIGNALS if c in df.columns]\n",
    "\n",
    "        for col in cols_to_filter:\n",
    "            s = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df[col] = (\n",
    "                s.rolling(window=kernel_size, center=True, min_periods=1)\n",
    "                 .median()\n",
    "                 .astype(float)\n",
    "            )\n",
    "\n",
    "        filename = os.path.basename(fp)\n",
    "        out_path = os.path.join(output_folder, filename)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\"[saved] {out_path}\")\n",
    "\n",
    "        if return_dict:\n",
    "            filtered_data[filename.replace(\".csv\", \"\")] = df\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# 运行\n",
    "filtered_data_dict = median_filter(\n",
    "    output_folder=\"/content/drive/My Drive/final_project/augmentation/medianfilter\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### windows sliding\n",
    "5s， 50% overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# 作为时序通道的列\n",
    "CANDIDATE_SIGNAL_COLS = [\n",
    "    \"Acc_X\",\"Acc_Y\",\"Acc_Z\",\n",
    "    \"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\",\n",
    "    \"FreeAcc_E\",\"FreeAcc_N\",\"FreeAcc_U\",\n",
    "]\n",
    "\n",
    "def _parse_meta_from_filename(fp: str):\n",
    "    base = os.path.basename(fp).replace(\".csv\",\"\")\n",
    "    m = re.match(r\"^T(\\d+)_([A-Za-z0-9]+(?:_WT\\d+)?)_(ankle|wrist)$\", base, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    subj = int(m.group(1))\n",
    "    act = m.group(2)\n",
    "    sens = m.group(3).lower()\n",
    "    return subj, act, sens\n",
    "\n",
    "def make_windows_from_folder(\n",
    "    folder: str,\n",
    "    signal_cols: List[str] = CANDIDATE_SIGNAL_COLS,\n",
    "    subject_col: str = \"Subject\",\n",
    "    activity_col: str = \"Activity\",\n",
    "    sensor_col: str = \"Sensor\",\n",
    "    time_col: str = \"PacketTime_ms\",  \n",
    "    fs_hz: int = 40,\n",
    "    window_sec: int = 5,\n",
    "    overlap: float = 0.5,\n",
    "    drop_na_windows: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "\n",
    "    assert 0 < overlap < 1\n",
    "    T    = int(round(window_sec * fs_hz))\n",
    "    step = max(1, int(round(T * (1.0 - overlap))))\n",
    "\n",
    "    X_list, y_list, g_list, meta_rows = [], [], [], []\n",
    "    csvs = sorted(glob.glob(os.path.join(folder, \"*.csv\")))\n",
    "    D_ref = None  \n",
    "\n",
    "    for fp in csvs:\n",
    "        df = pd.read_csv(fp)\n",
    "\n",
    "        # 从文件名获得 Subject/Activity/Sensor #\n",
    "        parsed = _parse_meta_from_filename(fp)\n",
    "        subj_val, act_val, sens_val = parsed\n",
    "        df[subject_col] = subj_val\n",
    "        df[activity_col] = act_val\n",
    "        df[sensor_col]  = sens_val\n",
    "\n",
    "        # 时间排序\n",
    "        if time_col not in df.columns:\n",
    "            raise ValueError(f\"{os.path.basename(fp)} 缺少时间列 {time_col}\")\n",
    "        df = df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "        cols = [c for c in signal_cols if c in df.columns]\n",
    "        if not cols:\n",
    "            print(f\"[warn] {os.path.basename(fp)} 无可用信号列，跳过。\")\n",
    "            continue\n",
    "\n",
    "        keep = cols + [subject_col, activity_col, sensor_col]\n",
    "        df = df[keep].copy()\n",
    "\n",
    "        for c in cols:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        # 按 (Subject, Sensor, Activity) 分组切窗\n",
    "        for (_subj, _sens, _act), g in df.groupby([subject_col, sensor_col, activity_col], sort=False):\n",
    "            arr = g[cols].to_numpy(dtype=float)\n",
    "            L, D_curr = arr.shape\n",
    "            if L < T:\n",
    "                continue\n",
    "\n",
    "            if D_ref is None:\n",
    "                D_ref = D_curr\n",
    "            elif D_curr != D_ref:\n",
    "                print(f\"{os.path.basename(fp)} 分组({_subj},{_sens},{_act}) 的通道数 D={D_curr} \"\n",
    "                      f\"与首个窗口 D={D_ref} 不一致，跳过该分组。\")\n",
    "                continue\n",
    "\n",
    "            # 切窗\n",
    "            start = 0\n",
    "            while start + T <= L:\n",
    "                win = arr[start:start+T] \n",
    "                if (not drop_na_windows) or np.isfinite(win).all():\n",
    "                    X_list.append(win)\n",
    "                    y_list.append(_act)\n",
    "                    g_list.append(_subj)  # LOSO 用 subject 分组\n",
    "                    meta_rows.append({\n",
    "                        \"file\": os.path.basename(fp),\n",
    "                        \"Subject\": _subj,\n",
    "                        \"Sensor\": _sens,\n",
    "                        \"Activity\": _act,\n",
    "                        \"start_pos\": int(start),\n",
    "                        \"end_pos\": int(start+T-1),\n",
    "                        \"T\": T,\n",
    "                        \"D\": D_ref\n",
    "                    })\n",
    "                start += step\n",
    "\n",
    "    X = np.stack(X_list, axis=0) if X_list else np.empty((0, T, len(CANDIDATE_SIGNAL_COLS)))\n",
    "    y = np.array(y_list)\n",
    "    groups = np.array(g_list)\n",
    "    meta = pd.DataFrame(meta_rows)\n",
    "    return X, y, groups, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_folder = \"/content/drive/My Drive/final_project/augmentation/medianfilter\"\n",
    "X, y, groups, meta = make_windows_from_folder(\n",
    "    folder=median_folder,\n",
    "    signal_cols=CANDIDATE_SIGNAL_COLS,\n",
    "    subject_col=\"Subject\",\n",
    "    activity_col=\"Activity\",\n",
    "    sensor_col=\"Sensor\",\n",
    "    fs_hz=40,\n",
    "    window_sec=5,\n",
    "    overlap=0.5,\n",
    ")\n",
    "print(\"X shape:\", X.shape) \n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"groups shape:\", groups.shape)\n",
    "print(meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 保存数据，后续能加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"../augmentation/windows/windows_5s_50pct_D9.npz\",\n",
    "    X=X, y=y, groups=groups\n",
    ")\n",
    "meta.to_csv(\"../augmentation/windows/windows_meta.csv\", index=False)\n",
    "\n",
    "# \n",
    "data = np.load(\"../augmentation/windows/windows_5s_50pct_D9.npz\")\n",
    "X, y, groups = data[\"X\"], data[\"y\"], data[\"groups\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q99kpCCcaT_L"
   },
   "source": [
    "#### HistGradientBoosting + PCA/SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10389955,
     "status": "ok",
     "timestamp": 1757105890171,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "PwNsEPmJq4XM",
    "outputId": "2bc524ce-95cf-43e2-8236-580813798107"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from scipy import interpolate\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (accuracy_score, f1_score, balanced_accuracy_score,\n",
    "                             classification_report, confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.base import SamplerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# \n",
    "import tsfel\n",
    "_cfg = tsfel.get_features_by_domain()\n",
    "TSFEL_CFG = {\"temporal\": _cfg[\"temporal\"], \"spectral\": _cfg[\"spectral\"]}\n",
    "\n",
    "SEED = 42\n",
    "rng_global = np.random.default_rng(SEED)\n",
    "\n",
    "# tsfel+4\n",
    "def _spectral_centroid_and_edge(x, fs_hz, edge_percent=0.9):\n",
    "    f, Pxx = welch(x, fs=fs_hz, nperseg=min(len(x), 256))\n",
    "    Pxx = np.clip(Pxx, 1e-12, None)\n",
    "    sc = float(np.sum(f * Pxx) / np.sum(Pxx))\n",
    "    cumsum = np.cumsum(Pxx) / np.sum(Pxx)\n",
    "    idx = int(np.searchsorted(cumsum, edge_percent))\n",
    "    idx = min(idx, len(f)-1)\n",
    "    sef = float(f[idx])\n",
    "    return sc, sef\n",
    "\n",
    "def _jerk_stats(x, fs_hz):\n",
    "    j = np.diff(x) * fs_hz\n",
    "    if j.size == 0:\n",
    "        return 0.0, 0.0\n",
    "    jm = float(np.mean(j))\n",
    "    js = float(np.std(j, ddof=1)) if j.size > 1 else 0.0\n",
    "    return jm, js\n",
    "\n",
    "def _coef_var(x):\n",
    "    m = float(np.mean(x))\n",
    "    if abs(m) < 1e-8:\n",
    "        return 0.0\n",
    "    return float(np.std(x, ddof=1) / abs(m))\n",
    "\n",
    "def featurize_windows_tsfel_plus(X, colnames, fs_hz):\n",
    "    rows = []\n",
    "    for n in range(X.shape[0]):\n",
    "        w = X[n] \n",
    "        df_w = pd.DataFrame(w, columns=colnames)\n",
    "\n",
    "        # TSFEL\n",
    "        tsfel_df = tsfel.time_series_features_extractor(\n",
    "            TSFEL_CFG, df_w, fs=fs_hz, window_size=None, verbose=0\n",
    "        )\n",
    "        tsfel_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # +4 \n",
    "        extras = {}\n",
    "        for d, name in enumerate(colnames):\n",
    "            x = w[:, d]\n",
    "            sc, sef = _spectral_centroid_and_edge(x, fs_hz, edge_percent=0.90)\n",
    "            if name.startswith(\"Acc_\") or name.startswith(\"FreeAcc_\"):\n",
    "                jm, js = _jerk_stats(x, fs_hz)\n",
    "            else:\n",
    "                jm, js = 0.0, 0.0\n",
    "            cv = _coef_var(x)\n",
    "            extras[f\"{name}_spec_centroid\"] = sc\n",
    "            extras[f\"{name}_spec_edge90\"] = sef\n",
    "            extras[f\"{name}_jerk_mean\"] = jm\n",
    "            extras[f\"{name}_jerk_std\"] = js\n",
    "            extras[f\"{name}_coef_var\"] = cv\n",
    "\n",
    "        rows.append(pd.concat([tsfel_df.iloc[0], pd.Series(extras)], axis=0))\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# augmentation\n",
    "def _find_triplets(colnames):\n",
    "    idx = {c:i for i,c in enumerate(colnames)}\n",
    "    out = []\n",
    "    for a,b,c in [(\"Acc_X\",\"Acc_Y\",\"Acc_Z\"),\n",
    "                  (\"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\"),\n",
    "                  (\"FreeAcc_E\",\"FreeAcc_N\",\"FreeAcc_U\")]:\n",
    "        if a in idx and b in idx and c in idx:\n",
    "            out.append((idx[a], idx[b], idx[c]))\n",
    "    return out\n",
    "\n",
    "def _random_rotation_matrix(rng, max_deg=20):\n",
    "    theta = np.deg2rad(rng.uniform(-max_deg, max_deg))\n",
    "    axis = rng.normal(size=3); axis /= (np.linalg.norm(axis)+1e-8)\n",
    "    x,y,z = axis; c,s = np.cos(theta), np.sin(theta)\n",
    "    return np.array([[c+x*x*(1-c),   x*y*(1-c)-z*s, x*z*(1-c)+y*s],\n",
    "                     [y*x*(1-c)+z*s, c+y*y*(1-c),   y*z*(1-c)-x*s],\n",
    "                     [z*x*(1-c)-y*s, z*y*(1-c)+x*s, c+z*z*(1-c)]], dtype=float)\n",
    "\n",
    "def aug_rotate(win, triplets, rng, max_deg=20):\n",
    "    if not triplets: return win\n",
    "    R = _random_rotation_matrix(rng, max_deg=max_deg)\n",
    "    out = win.copy()\n",
    "    for i,j,k in triplets:\n",
    "        out[:, [i,j,k]] = out[:, [i,j,k]] @ R.T\n",
    "    return out\n",
    "\n",
    "def aug_permute(win, rng, min_seg=3, max_seg=5):\n",
    "    T = win.shape[0]\n",
    "    K = rng.integers(min_seg, max_seg+1)\n",
    "    idx = np.linspace(0, T, K+1).astype(int)\n",
    "    segs = [win[idx[i]:idx[i+1]] for i in range(K)]\n",
    "    return np.concatenate([segs[o] for o in rng.permutation(K)], axis=0)\n",
    "\n",
    "def aug_timewarp(win, rng, sigma=0.2, knots=4):\n",
    "    T, D = win.shape\n",
    "    t_src = np.linspace(0, 1, T)\n",
    "    knot_x = np.linspace(0, 1, knots+2)\n",
    "    perturb = rng.normal(0, sigma, size=knots)\n",
    "    knot_y = np.r_[0, np.cumsum(perturb)/max(1,knots), 1.0]\n",
    "    knot_y = np.sort(np.clip(knot_y, 0, 1))\n",
    "    spl = interpolate.UnivariateSpline(knot_x, knot_y, k=3, s=0)\n",
    "    t_new = spl(t_src)\n",
    "    t_new = (t_new - t_new.min()) / (t_new.max() - t_new.min() + 1e-8)\n",
    "    out = np.zeros_like(win)\n",
    "    for d in range(D):\n",
    "        spld = interpolate.UnivariateSpline(t_src, win[:,d], k=3, s=0)\n",
    "        out[:,d] = spld(t_new)\n",
    "    return out\n",
    "\n",
    "def aug_jitter(win, rng, percent=0.02):\n",
    "    noise = rng.normal(0, 1, size=win.shape) * (np.std(win, axis=0, ddof=1) * percent + 1e-8)\n",
    "    return win + noise\n",
    "\n",
    "def aug_scaling(win, rng, low=0.9, high=1.1):\n",
    "    scale = rng.uniform(low, high, size=(1, win.shape[1]))\n",
    "    return win * scale\n",
    "\n",
    "def aug_magwarp(win, rng, sigma=0.2, knots=4):\n",
    "    T, D = win.shape\n",
    "    t = np.linspace(0, 1, T)\n",
    "    knot_x = np.linspace(0, 1, knots+2)\n",
    "    warp = np.zeros((T, D))\n",
    "    for d in range(D):\n",
    "        pert = rng.normal(0, sigma, size=knots)\n",
    "        knot_y = np.r_[0, np.cumsum(pert)/max(1,knots), 0]\n",
    "        spl = interpolate.UnivariateSpline(knot_x, knot_y, k=3, s=0)\n",
    "        warp[:, d] = 1.0 + spl(t)  \n",
    "    return win * warp\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class WindowAugmenter(BaseEstimator):\n",
    "    _parameter_constraints: dict = {}\n",
    "\n",
    "    def __init__(self, colnames, fs_hz=40,\n",
    "                 do_rot=True, rot_max_deg=20,\n",
    "                 do_timewarp=False, tw_sigma=0.2, tw_knots=4,\n",
    "                 do_permute=False, perm_min=3, perm_max=5,\n",
    "                 do_jitter=False, jitter_pct=0.02,\n",
    "                 do_scaling=False, scale_low=0.9, scale_high=1.1,\n",
    "                 do_magwarp=False, mw_sigma=0.2, mw_knots=4,\n",
    "                 per_class_strategy=\"q80\",\n",
    "                 random_state=42):\n",
    "        self.colnames = colnames\n",
    "        self.fs_hz = fs_hz\n",
    "        self.do_rot = do_rot; self.rot_max_deg = rot_max_deg\n",
    "        self.do_timewarp = do_timewarp; self.tw_sigma = tw_sigma; self.tw_knots = tw_knots\n",
    "        self.do_permute = do_permute; self.perm_min = perm_min; self.perm_max = perm_max\n",
    "        self.do_jitter = do_jitter; self.jitter_pct = jitter_pct\n",
    "        self.do_scaling = do_scaling; self.scale_low = scale_low; self.scale_high = scale_high\n",
    "        self.do_magwarp = do_magwarp; self.mw_sigma = mw_sigma; self.mw_knots = mw_knots\n",
    "        self.per_class_strategy = per_class_strategy\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.triplets_ = _find_triplets(self.colnames)\n",
    "        self.rng_ = np.random.default_rng(self.random_state)\n",
    "        if (y is None) or (self.per_class_strategy == \"none\"):\n",
    "            self.ratio_ = None\n",
    "        else:\n",
    "            cnt = pd.Series(y).value_counts()\n",
    "            if self.per_class_strategy == \"q80\":\n",
    "                target = int(min(cnt.max(), cnt.quantile(0.8)))\n",
    "            elif self.per_class_strategy == \"max\":\n",
    "                target = int(cnt.max())\n",
    "            else:\n",
    "                target = None\n",
    "            self.ratio_ = None if target is None else {c: max(1, int(np.ceil(target/c))) for c in cnt.index}\n",
    "        return self\n",
    "\n",
    "    def _augment_one(self, w):\n",
    "        rng = self.rng_\n",
    "        out = w.copy()\n",
    "        if self.do_rot:\n",
    "            out = aug_rotate(out, self.triplets_, rng, max_deg=self.rot_max_deg)\n",
    "        if self.do_timewarp:\n",
    "            out = aug_timewarp(out, rng, sigma=self.tw_sigma, knots=self.tw_knots)\n",
    "        if self.do_permute:\n",
    "            out = aug_permute(out, rng, min_seg=self.perm_min, max_seg=self.perm_max)\n",
    "        if self.do_jitter:\n",
    "            out = aug_jitter(out, rng, percent=self.jitter_pct)\n",
    "        if self.do_scaling:\n",
    "            out = aug_scaling(out, rng, low=self.scale_low, high=self.scale_high)\n",
    "        if self.do_magwarp:\n",
    "            out = aug_magwarp(out, rng, sigma=self.mw_sigma, knots=self.mw_knots)\n",
    "        return out\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        if getattr(self, \"ratio_\", None) is None:\n",
    "            return X, y\n",
    "        X_aug, y_aug = [X], [y]\n",
    "        y_arr = np.asarray(y)\n",
    "        for cls, mult in self.ratio_.items():\n",
    "            if mult <= 1:\n",
    "                continue\n",
    "            idx = np.where(y_arr == cls)[0]\n",
    "            need = (mult - 1) * len(idx)\n",
    "            if need <= 0 or len(idx) == 0:\n",
    "                continue\n",
    "            pick = self.rng_.choice(idx, size=need, replace=True)\n",
    "            aug_list = [self._augment_one(X[i]) for i in pick]\n",
    "            X_aug.append(np.stack(aug_list))\n",
    "            y_aug.append(np.full(len(aug_list), cls, dtype=y_arr.dtype))\n",
    "        return np.concatenate(X_aug, axis=0), np.concatenate(y_aug, axis=0)\n",
    "\n",
    "class WindowFeaturizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, colnames, fs_hz=40):\n",
    "        self.colnames = colnames; self.fs_hz = fs_hz\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        Feat = featurize_windows_tsfel_plus(X, self.colnames, self.fs_hz)\n",
    "        A = Feat.values    # 交给后续 standardscaler,selector\n",
    "        A = np.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0) \n",
    "        return A\n",
    "\n",
    "# load data & merge M10\n",
    "data = np.load(\"/content/drive/My Drive/final_project/augmentation/windows/windows_5s_50pct_D9.npz\")\n",
    "X, y, groups = data[\"X\"], data[\"y\"].astype(str), data[\"groups\"]\n",
    "\n",
    "def collapse_walk_labels(y_arr):\n",
    "    y_out = y_arr.copy().astype(str)\n",
    "    mask = np.char.startswith(y_out, \"M10\")\n",
    "    y_out[mask] = \"M10\"\n",
    "    return y_out\n",
    "\n",
    "y = collapse_walk_labels(y)\n",
    "\n",
    "signal_cols = [\"Acc_X\",\"Acc_Y\",\"Acc_Z\",\"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\",\"FreeAcc_E\",\"FreeAcc_N\",\"FreeAcc_U\"]\n",
    "fs_hz = 40\n",
    "\n",
    "# split - subject level\n",
    "train_subjects = np.array([29, 28, 24, 21, 3, 26, 14, 25, 12, 18, 5, 11])\n",
    "val_subjects   = np.array([17, 13, 22, 23, 20])\n",
    "test_subjects  = np.array([4, 2, 1, 19])\n",
    "\n",
    "tr_mask = np.isin(groups, train_subjects)\n",
    "va_mask = np.isin(groups, val_subjects)\n",
    "te_mask = np.isin(groups, test_subjects)\n",
    "\n",
    "X_tr, y_tr, g_tr = X[tr_mask], y[tr_mask], groups[tr_mask]\n",
    "X_va, y_va = X[va_mask], y[va_mask]\n",
    "X_te, y_te = X[te_mask], y[te_mask]\n",
    "\n",
    "print(\"Train/Val/Test windows:\", X_tr.shape, X_va.shape, X_te.shape)\n",
    "\n",
    "# Pipeline\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "augmenter = WindowAugmenter(\n",
    "    colnames=signal_cols, fs_hz=fs_hz,\n",
    "    do_rot=True,  rot_max_deg=20,\n",
    "    do_timewarp=True, tw_sigma=0.2, tw_knots=4,\n",
    "    do_permute=False,\n",
    "    do_jitter=False,\n",
    "    do_scaling=False,\n",
    "    do_magwarp=False,\n",
    "    per_class_strategy=\"q80\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe = ImbPipeline([\n",
    "    (\"augment\", augmenter), \n",
    "    (\"featurize\", WindowFeaturizer(signal_cols, fs_hz)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"selector\", PCA(random_state=SEED)),\n",
    "    # (\"selector\", KBestFlex(score_func=f_classif, k=0.6)),  \n",
    "    (\"clf\", HistGradientBoostingClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"augment__rot_max_deg\": [20], \n",
    "    \"augment__do_timewarp\": [True], \n",
    "    \"augment__tw_sigma\": [0.2],\n",
    "    \"augment__tw_knots\": [4],  \n",
    "    \"augment__per_class_strategy\": [\"q80\"],\n",
    "    \"selector__n_components\": [0.8],  # 0.9, 0.95, 0.7\n",
    "    \"clf__learning_rate\": [0.01],       \n",
    "    \"clf__max_depth\": [8, 6], \n",
    "    \"clf__max_iter\": [800], \n",
    "    \"clf__min_samples_leaf\": [25],   \n",
    "    \"clf__l2_regularization\": [10.0],   \n",
    "    \"clf__early_stopping\": [True],  \n",
    "}\n",
    "\n",
    "# --------------------------- RF ----------------------------\n",
    "# pipe_rf_kbest = ImbPipeline([\n",
    "#     (\"augment\", augmenter),\n",
    "#     (\"featurize\", WindowFeaturizer(signal_cols, fs_hz)),\n",
    "#     (\"scale\", StandardScaler()),\n",
    "#     (\"selector\", KBestFlex(score_func=f_classif, k=0.6)), \n",
    "#     (\"clf\", RandomForestClassifier(\n",
    "#         class_weight=\"balanced_subsample\",  \n",
    "#         n_jobs=-1, random_state=SEED\n",
    "#     ))\n",
    "# ])\n",
    "# param_grid_rf_kbest = {\n",
    "#     \"augment__rot_max_deg\": [20],\n",
    "#     \"augment__do_timewarp\": [True],\n",
    "#     \"augment__tw_sigma\": [0.2],\n",
    "#     \"augment__tw_knots\": [4],\n",
    "#     \"augment__per_class_strategy\": [\"q80\"],\n",
    "#     \"selector__k\": [0.6],\n",
    "#     \"clf__n_estimators\": [600],        # [300, 500]\n",
    "#     \"clf__max_depth\": [10],     \n",
    "#     \"clf__min_samples_split\": [4],   # [2, 4]\n",
    "#     \"clf__min_samples_leaf\": [8],    # [2, 8]\n",
    "#     \"clf__max_features\": [\"sqrt\"],     # [\"sqrt\", \"log2\"]\n",
    "# }\n",
    "# --------------------------- RF ----------------------------\n",
    "\n",
    "# --------------------------- LightGBM ----------------------------\n",
    "# from lightgbm import LGBMClassifier\n",
    "# pipe_lgbm_pca = ImbPipeline([\n",
    "#     (\"augment\", augmenter),\n",
    "#     (\"featurize\", WindowFeaturizer(signal_cols, fs_hz)),\n",
    "#     (\"scale\", StandardScaler()),\n",
    "#     (\"selector\", PCA(random_state=SEED)),\n",
    "#     (\"clf\", LGBMClassifier(\n",
    "#         objective=\"multiclass\", num_class=len(np.unique(y_tr)),\n",
    "#         random_state=SEED, n_jobs=-1\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# param_grid_lgbm_pca = {\n",
    "#     \"augment__per_class_strategy\": [\"q80\"],\n",
    "#     \"selector__n_components\": [0.8],\n",
    "#     \"clf__learning_rate\": [0.05],          # 0.01,\n",
    "#     \"clf__n_estimators\": [800],         # 600,\n",
    "#     \"clf__max_depth\": [8],          # -1 表示不限制, -1, 6,\n",
    "#     \"clf__num_leaves\": [63],           # 31,\n",
    "#     \"clf__min_child_samples\": [100],       # 50,\n",
    "#     \"clf__subsample\": [1.0],          # bagging  # 0.7,\n",
    "#     \"clf__colsample_bytree\": [1.0],       # 0.7,\n",
    "#     \"clf__reg_lambda\": [10.0],             # 0.0,\n",
    "# }\n",
    "# --------------------------- LightGBM ----------------------------\n",
    "\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=logo.split(X_tr, y_tr, g_tr), \n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=2,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "gs.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"Best LOSO acc on Dev(Train):\", gs.best_score_)\n",
    "\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "from joblib import dump\n",
    "dump(best_model, '/content/drive/My Drive/final_project/augmentation/modelSave/best_model_hist.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSTzf829rsOQ"
   },
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "executionInfo": {
     "elapsed": 402920,
     "status": "ok",
     "timestamp": 1757106405969,
     "user": {
      "displayName": "Chen Yi",
      "userId": "15055353069078412577"
     },
     "user_tz": -60
    },
    "id": "RGCy_14OroG1",
    "outputId": "5d5a0e65-446c-4e54-fddb-4362d97efcfb"
   },
   "outputs": [],
   "source": [
    "\n",
    "yhat_va = best_model.predict(X_va)\n",
    "\n",
    "labels_va = best_model.named_steps[\"clf\"].classes_  \n",
    "print(\"VAL Acc: %.3f | Macro-F1: %.3f | BalAcc: %.3f\" % (\n",
    "    accuracy_score(y_va, yhat_va),\n",
    "    f1_score(y_va, yhat_va, average=\"macro\"),\n",
    "    balanced_accuracy_score(y_va, yhat_va)\n",
    "))\n",
    "\n",
    "print(\"\\nValidation report:\\n\", classification_report(y_va, yhat_va, labels=labels_va, target_names=labels_va))\n",
    "\n",
    "# cm_va = confusion_matrix(y_va, yhat_va, labels=labels_va)\n",
    "# ConfusionMatrixDisplay(cm_va, display_labels=labels_va).plot(xticks_rotation=45, cmap=\"Blues\", values_format=\"d\")\n",
    "# plt.title(\"Confusion Matrix - Validation\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "########################\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusions(y_true, y_pred, labels, title_prefix=\"Validation\"):\n",
    "    cm_counts = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(cm_counts, display_labels=labels)\n",
    "    disp.plot(xticks_rotation=45, cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(f\"Confusion Matrix — {title_prefix} (Counts)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    cm_norm = confusion_matrix(y_true, y_pred, labels=labels, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(cm_norm, display_labels=labels)\n",
    "    disp.plot(xticks_rotation=45, cmap=\"Blues\", values_format=\".2f\")\n",
    "    plt.title(f\"Confusion Matrix — {title_prefix} (Normalized)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "labels_va = best_model.named_steps[\"clf\"].classes_ \n",
    "plot_confusions(y_va, yhat_va, labels_va, title_prefix=\"Validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wzXBWO0vCvF"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG9bCtJjrpiz"
   },
   "source": [
    "##### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vy9GUFiErrPx"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Dev(Train+Val) 重训 -- Test 评估\n",
    "dev_mask = np.isin(groups, np.r_[train_subjects, val_subjects])\n",
    "X_dev, y_dev = X[dev_mask], y[dev_mask]\n",
    "\n",
    "final_model = clone(best_model)  \n",
    "final_model.fit(X_dev, y_dev)  \n",
    "\n",
    "yhat_te = final_model.predict(X_te)\n",
    "\n",
    "labels_te = final_model.named_steps[\"clf\"].classes_\n",
    "print(\"TEST Acc: %.3f | Macro-F1: %.3f | BalAcc: %.3f\" % (\n",
    "    accuracy_score(y_te, yhat_te),\n",
    "    f1_score(y_te, yhat_te, average=\"macro\"),\n",
    "    balanced_accuracy_score(y_te, yhat_te)\n",
    "))\n",
    "\n",
    "print(\"\\nTest report:\\n\", classification_report(y_te, yhat_te, labels=labels_te, target_names=labels_te))\n",
    "\n",
    "# cm_te = confusion_matrix(y_te, yhat_te, labels=labels_te)\n",
    "# ConfusionMatrixDisplay(cm_te, display_labels=labels_te).plot(xticks_rotation=45, cmap=\"Blues\", values_format=\"d\")\n",
    "# plt.title(\"Confusion Matrix - Test\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "## test 集的两版 confusion matrix\n",
    "labels_te = final_model.named_steps[\"clf\"].classes_\n",
    "plot_confusions(y_te, yhat_te, labels_te, title_prefix=\"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0F9oCWgJhPl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iF4KyRMlJhMB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOiTyVFt2jWMeDbr+A3CrS0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
